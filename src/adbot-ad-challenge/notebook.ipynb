{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to Adbot Challenge! The objective of this challenge is to accurately predict the number of clicks a clientâ€™s ad receives, one and two weeks into the future (in digital marketing, clicks refer to when someone views the advert and follows one of the hyperlinks in that advert). Not only that, the challenge also requires one to find the most significant features on each prediction. Therefore, the best solution will not only produce an accurate prediction, but also the interpretation.\n",
    "\n",
    "# Loading Libraries\n",
    "\n",
    "There are 10 libraries we are using in this notebook:\n",
    "\n",
    "1. Pandas for storing dataframe and creating new features;\n",
    "2. Polars strictly for preprocessing and wrangling datasets;\n",
    "3. Numpy strictly for creating array and setting seed value;\n",
    "4. Matplotlib for visualization;\n",
    "5. Seaborn for visualization;\n",
    "5. SHAP for interpretation;\n",
    "6. Holidays for determining whether a date is an holiday or not;\n",
    "7. Category Encoders for categorical encoding;\n",
    "8. Scikit-Learn for building pipeline;\n",
    "9. LightGBM for machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shap\n",
    "\n",
    "import holidays\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "FILEDIR = Path('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Datasets\n",
    "\n",
    "The dataset we have contains the observation for each ad that Adbot's client have. However, what we need is to find the total ad clicks that each client get for a single day. Therefore, we need to aggregate the train data based on client ID and date. The features that we use from the original dataset are\n",
    "\n",
    "1. client ID,\n",
    "2. date,\n",
    "3. ad type,\n",
    "4. currency,\n",
    "5. call type,\n",
    "6. call status\n",
    "7. duration, and\n",
    "8. display location.\n",
    "\n",
    "We also don't have the test dataset for this challenge. However, we are given sample submission. This is helpful for us as we can wrangle the sample submission instead of building test dataset from scratch. One thing to note is that we are not given any features for test dataset other than date and ID, as those two are the only known features in the future. Therefore, we use forward filling to create features in dataset.\n",
    "\n",
    "Note 1: Categorical features mode aggregation process is non-deterministic due to the possibility of a feature having more than one mode, and every run produces different order.\n",
    "Note 2: `duration` is recognized as string by Polars. Ideally, we changed it to float. However, we will keep it as string because that was how the best submission was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_preprocessor = [\n",
    "    pl.col('date').cast(pl.Date),\n",
    "    pl.col('start_time').str.strip_chars_end().str.to_datetime('%Y-%m-%d %H:%M:%S').cast(pl.Datetime),\n",
    "    pl.col('end_time').str.strip_chars_end().str.to_datetime('%Y-%m-%d %H:%M:%S').cast(pl.Datetime),\n",
    "]\n",
    "\n",
    "datetime_calculator = [\n",
    "    pl.col('date').cast(pl.Float32).alias('date_int')\n",
    "]\n",
    "\n",
    "unaggregated_train = pl.scan_csv(FILEDIR / 'input' / 'Train.csv').\\\n",
    "    with_columns(datetime_preprocessor).\\\n",
    "    with_columns(datetime_calculator)\n",
    "\n",
    "train = unaggregated_train.group_by('ID', 'date').agg(\n",
    "    pl.col('date_int').median().cast(pl.Int16),\n",
    "    pl.col('clicks').sum().cast(pl.Float32),\n",
    "    pl.col(pl.String).mode().map_elements(lambda x: x[0], return_dtype = pl.String),\n",
    ").sort(['date', 'ID'])\n",
    "\n",
    "test = pl.scan_csv(FILEDIR / 'input' / 'SampleSubmission.csv').\\\n",
    "    with_columns(\n",
    "        pl.col('ID').str.head(27),\n",
    "        pl.col('ID').str.tail(10).str.to_datetime('%Y_%m_%d').cast(pl.Date).alias('date')\n",
    "    ).with_columns(\n",
    "        pl.col('date').cast(pl.Int16).alias('date_int'),\n",
    "    ).with_columns(\n",
    "        clicks = None\n",
    "    ).with_columns(pl.col('clicks').cast(pl.Float32)).select(pl.col('ID', 'date', 'date_int', 'clicks')).join(\n",
    "        train,\n",
    "        on = ['ID', 'date', 'date_int', 'clicks'],\n",
    "        how = 'left'\n",
    "    )\n",
    "\n",
    "full_dataset = pl.concat(\n",
    "    [train, test], how = 'vertical_relaxed'\n",
    ").sort(\n",
    "    ['ID', 'date']\n",
    ").with_columns(\n",
    "    pl.exclude('clicks').forward_fill()\n",
    ")\n",
    "\n",
    "train = full_dataset.filter(~pl.col('clicks').is_null()).sort(['date', 'ID'])\n",
    "test = full_dataset.filter(pl.col('clicks').is_null()).drop('clicks').collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.collect().to_pandas()\n",
    "y = X.pop('clicks')\n",
    "\n",
    "cat_features = list(X.select_dtypes('object'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We only create new feature: holiday. If a date is an holiday, it will have a value of 1. Otherwise, it will be 0. The feature engineering will be done in a function called `holiday_generator`, and we build Scikit-Learn wrapper with `FunctionTransformer` so we can implement it in Scikit-Learn pipeline.\n",
    "\n",
    "Note: We only generate holidays in South Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_generator(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    years = list(df.date.dt.year.unique())\n",
    "        \n",
    "    df_copy = df.copy()\n",
    "        \n",
    "    holiday_map = holidays.country_holidays('ZA', years = years)\n",
    "        \n",
    "    df_copy['holiday_name'] = df_copy.date.map(holiday_map)\n",
    "    df_copy['is_holiday'] = np.where(df_copy.holiday_name.notnull(), 1, 0)\n",
    "    df_copy = df_copy.drop(['holiday_name', 'date'], axis = 1)\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "HolidayGenerator = FunctionTransformer(holiday_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We build a preprocessor pipeline consisting of `HolidayGenerator` to generate holiday indicator and `MEstimateEncoder` to encode categorical features. As for our machine learning model, we will use LightGBM with DART bootstrapping and GOSS data sampling strategy. From this, we preprocess our predictors in the train dataset and the test dataset.\n",
    "\n",
    "Note: We could have built a single pipeline from preprocessing until predicting. However, due to ease of interpreting our model with SHAP, we chose to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessor = make_pipeline(\n",
    "    HolidayGenerator,\n",
    "    MEstimateEncoder(cat_features)\n",
    ")\n",
    "\n",
    "Preprocessor.fit(X, y)\n",
    "preprocessed_X = Preprocessor.transform(X)\n",
    "preprocessed_test = Preprocessor.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(\n",
    "    random_state = seed,\n",
    "    verbose = -1,\n",
    "    boosting_type = 'dart', \n",
    "    data_sample_strategy = 'goss'\n",
    ")\n",
    "\n",
    "model.fit(preprocessed_X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting and Calculating SHAP Values\n",
    "\n",
    "In order to interpret the result with SHAP, we build our explainer based on our fitted LightGBM and preprocessed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model, preprocessed_X)\n",
    "test_explanation = explainer(preprocessed_test)\n",
    "shap_value = explainer.shap_values(preprocessed_test)\n",
    "\n",
    "predictions = model.predict(preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Submission Files\n",
    "\n",
    "We create submission file based on our predictions and most important features. For the purpose of this competition, we limit the list of most important features to 5 features. Top 5 features are determined by the biggest absolute value of its SHAP values.\n",
    "\n",
    "Note: If we use less than 5 features, the least important features will be `NULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_columns = [f'feature_{i}' for i in range(1, shap_value.shape[1] + 1 if shap_value.shape[1] < 5 else 6)]\n",
    "\n",
    "submission = pd.concat([\n",
    "    pd.Series(test.ID + '_' + test.date.astype(str).str.replace('-', '_'), name = 'ID'),\n",
    "    pd.Series(predictions, name = 'target'),\n",
    "    pd.DataFrame((-1 * np.abs(shap_value)).argsort()[:, :5], columns = importance_columns)\n",
    "], axis = 1)\n",
    "\n",
    "for col in importance_columns:\n",
    "    submission[col] = submission[col].apply(lambda x: preprocessed_X.columns[x])\n",
    "\n",
    "if submission.shape[1] < 7:\n",
    "    for i in range(submission.shape[1] - 1, 6):\n",
    "        submission[f'feature_{i}'] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(FILEDIR / 'output' / 'submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "In this section, we will see how much the top 5 important features change independently to improve the total ad clicks. We will obtain the SHAP values for the top 5 features and concat it with our submission dataframe for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_values = pd.DataFrame(shap_value).apply(lambda row: pd.Series(sorted(row, key = lambda row: -1 * np.abs(row))), axis = 1).iloc[:, :5]\n",
    "top_5_values.columns = [col + '_values' for col in list(submission.iloc[:, 2:])]\n",
    "sensitivity_table = pd.concat([submission, top_5_values], axis = 1)\n",
    "sensitivity_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, is an example of visualization of SHAP value for a single observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15), dpi = 300)\n",
    "plt.title(\"SHAP Value for a Client in a Day\", size = 15, weight = 'bold')\n",
    "shap.plots.waterfall(test_explanation[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four plots shown below. The first two measures the impact of each feature on each target, while the last two shows the variance on the level of importance on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15), dpi = 300)\n",
    "plt.title('Impact Measurement on Each Feature per Observation', size = 13, weight = 'bold')\n",
    "shap.plots.beeswarm(test_explanation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15), dpi = 300)\n",
    "plt.title('Impact Measurement on Each Feature per Clustered Observation', size = 13, weight = 'bold')\n",
    "shap.plots.heatmap(test_explanation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 20), dpi = 300)\n",
    "importance_counter = submission\\\n",
    "    .iloc[:, 2:]\\\n",
    "    .melt()\\\n",
    "    .rename(\n",
    "        {'variable' : 'importance_level', 'value' : 'features'},\n",
    "        axis = 1\n",
    "    )\n",
    "importance_counter['importance_level'] = importance_counter['importance_level']\\\n",
    "    .str\\\n",
    "    .replace('feature_1', '1st')\\\n",
    "    .replace('feature_2', '2nd')\\\n",
    "    .replace('feature_3', '3rd')\\\n",
    "    .replace('feature_4', '4th')\\\n",
    "    .replace('feature_5', '5th')\n",
    "sns.countplot(\n",
    "    data = importance_counter, \n",
    "    y = 'features', \n",
    "    hue = 'importance_level'\n",
    ")\n",
    "plt.title('Importance Level Count per Feature', size = 25, weight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize = (16, 25), dpi = 300)\n",
    "\n",
    "for i, column in enumerate(importance_columns):\n",
    "\n",
    "    ax[i][0].pie(\n",
    "        submission[column].value_counts(), \n",
    "        shadow = True, \n",
    "        explode = [.1 for i in range(submission[column].nunique())], \n",
    "        autopct = '%1.f%%',\n",
    "        textprops = {'size' : 14, 'color' : 'white'},\n",
    "    )\n",
    "\n",
    "    sns.countplot(\n",
    "        data = submission, \n",
    "        y = column, \n",
    "        ax = ax[i][1], \n",
    "        order = submission[column].value_counts().index\n",
    "    )\n",
    "    ax[i][1].yaxis.label.set_size(20)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    ax[i][1].set_xlabel('Count', fontsize = 15)\n",
    "    ax[i][1].set_ylabel(f'Important Feature {i+1}', fontsize = 15)\n",
    "    plt.xticks(fontsize = 12)\n",
    "\n",
    "fig.suptitle('Feature Count per Importance Level\\n\\n', fontsize = 25, fontweight = 'bold')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
